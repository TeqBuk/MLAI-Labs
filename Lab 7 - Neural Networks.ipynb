{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Convolutional Neural Networks for Image Classification\n",
    "[**Haiping Lu**](http://staffwww.dcs.shef.ac.uk/people/H.Lu/) -  [COM4509/6509 MLAI2020](https://github.com/maalvarezl/MLAI) @ The University of Sheffield\n",
    "\n",
    "**Sources**: This notebook is based on [the CIFAR10 Pytorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py), [the CNN notebook from Lisa Zhang](https://www.cs.toronto.edu/~lczhang/360/lec/w04/convnet.html), and Lab 2 and Lab 3 of my [SimplyDeep](https://github.com/haipinglu/SimplyDeep/) notebooks.\n",
    "\n",
    "There are *six* questions in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "* To perform image classification using convolutional neural network in PyTorch.\n",
    "\n",
    "**Suggested reading**: \n",
    "* [Autograd tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)\n",
    "* [Convolutional neural network - Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n",
    "* [Feature/representation learning - Wikipedia](https://en.wikipedia.org/wiki/Feature_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why\n",
    "\n",
    "The fast rising of [deep learning](https://en.wikipedia.org/wiki/Deep_learning) starts on 30 September 2012, when a convolutional neural network (CNN) called [AlexNet](https://en.wikipedia.org/wiki/AlexNet) achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge, more than **10.8 percentage** points lower than that of the runner up. This is considered a breakthrough and has grabbed the attention of increasing number of researchers, practioners, and the general public. Since then, deep learning has penetrated to many research and application areas. AlexNet contained **eight layers**. In 2015, it was outperformed by a very deep CNN with **over 100 layers** from Microsoft in the ImageNet 2015 contest. It will be interesting to take a look at the image classification task and a CNN that can do the job well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review of Autograd: Automatic Differentiation\n",
    "\n",
    "In the previous lab, we briefly covered **Tensor** and **Computational Graph**. We have actually used **Autograd** already. Here, we learn the basics below, a condensed and modified version of the original [PyTorch tutorial on Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)\n",
    "\n",
    "#### Why differentiation is important? \n",
    "\n",
    "This is because it is a key procedure in **optimisation** to find the optimial solution of a loss function. The process of learning/training aims to minimise a predefined loss.\n",
    "\n",
    "#### How automatic differentiation is done in PyTorch?\n",
    "The PyTorch ``autograd`` package makes differentiation (almost) transparent to you by providing automatic differentiation for all operations on Tensors, unless you do not want it (to save time and space). \n",
    "\n",
    "A ``torch.Tensor`` type variable has an attribute ``.requires_grad``. Setting this attribute ``True`` tracks (but not computes yet) all operations on it. After we define the forward pass, and hence the *computational graph*, we call ``.backward()`` and all the gradients will be computed automatically and accumulated into the ``.grad`` attribute. \n",
    "\n",
    "This is made possible by the [**chain rule of differentiation**](https://en.wikipedia.org/wiki/Chain_rule).\n",
    "\n",
    "#### How to stop automatic differentiation (e.g., because it is not needed)\n",
    "Calling method ``.detach()`` of a tensor will detach it from the computation history. We can also wrap the code block in ``with torch.no_grad():`` so all tensors in the block do not track the gradients, e.g., in the test/evaluation stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 1\n",
    "\n",
    "What is the benefit of stopping automatic differentiation when it is not needed?\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "``Tensor``s are connected by ``Function`` to build an acyclic *computational graph* to encode a complete history of computation. The ``.grad_fn`` attribute of a tensor references a ``Function`` created\n",
    "the ``Tensor``, i.e., this ``Tensor`` is the output of its ``.grad_fn`` in the computational graph.\n",
    "\n",
    "Learn more about autograd by referring to the [documentation on autograd](https://pytorch.org/docs/stable/autograd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Image Data - CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries \n",
    "\n",
    "Get ready by importing commonly used APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "The [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) has ten classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "#### Loading and normalizing CIFAR10\n",
    "\n",
    "The output of torchvision datasets (after loading) are PILImage images of range [0, 1]. \n",
    "\n",
    "Check out the `torchvision.transforms` API for [here](https://pytorch.org/docs/stable/torchvision/transforms.html) (search for `ToTensor` and `Normalize`).\n",
    "\n",
    "`transforms.ToTensor()` Convert a `PIL` Image or `numpy.ndarray` (H x W x C) in the range [0, 255]  to torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "\n",
    "`transforms.Normalize` normalizes a tensor image with mean and standard deviation. Given mean: ($M1,...,Mn$) and std: ($S1,..,Sn$) for $n$ channels, this transform will normalize each channel of the input `torch.*Tensor` as $input[channel] = (input[channel] - mean[channel]) / std[channel]$\n",
    "\n",
    "`torch.utils.data.DataLoader` combines a dataset and a sampler, and provides an iterable over the given dataset. See [API here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "\n",
    "#### Batching\n",
    "\n",
    "We want to use more than one images at one time. That way, we can compute the *average* loss across a **mini-batch** of $n$ *multiple* images, and take a step to optimize the *average* loss. The average loss across multiple training inputs is going to be **less \"noisy\"** than the loss for a single input, and is less likely to provide \"bad information\" because of a \"bad\" input. The number $n$ is called the **batch size**.\n",
    "\n",
    "The actual batch size that we choose depends on many things. We want our batch\n",
    "size to be large enough to not be too \"noisy\", but not so large as to make each\n",
    "iteration too expensive to run.\n",
    "\n",
    "People often choose batch sizes of the form $n=2^k$ so that it is easy to half\n",
    "or double the batch size. \n",
    "\n",
    "####  Epoch\n",
    "The way `DataLoader` works is that it randomly groups the training data into **mini-batches**\n",
    "with the appropriate batch size. Each data point belongs to only one mini-batch. When there\n",
    "are no more mini-batches left, the loop terminates.\n",
    "\n",
    "In general, we may wish to train the network for longer. We may wish to use each training data\n",
    "point more than once. In other words, we may wish to train a neural network for more than\n",
    "**one epoch**. An **epoch** is a measure of the number of times all training data is used\n",
    "once to update the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batchSize=4\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "#Load the training data\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchSize,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "#Load the test data\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchSize, \n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print('Training set size:', len(trainset))\n",
    "print('Test set size:',len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data has been downloaded at the `data` directory. Because the filesize is large, we do not upload it to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize back to range [0, 1]\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0))) #rearrange dimensions to numpy format for disply\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next() #Get one batch (4 here)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batchSize)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Architecture of a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical CNN architecture:\n",
    "\n",
    "![Typical CNN architecture](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)\n",
    "\n",
    "Let us look at the CNN in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution layer - with a shared kernel/filter\n",
    "<center>\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/360/lec/w04/imgs/math_kernel.png\" width=\"100px\" style=\"margin:0; display:inline\">\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/360/lec/w04/imgs/math_conv.png\" width=\"300px\" style=\"margin:0; display:inline\">\n",
    "</center>\n",
    "\n",
    "The light blue grid (middle) is the *input* that we are given, e.g., a 5 pixel by 5 pixel greyscale image. The grey grid (left) is a **convolutional kernel/filter** of size $3 \\times 3$, containing the *parameters* of this neural network layer.\n",
    "\n",
    "To compute the output, we superimpose the kernel on a region of the image. \n",
    "Let's start at the top left, in the dark blue region. The small numbers in the\n",
    "bottom right corner of each grid element corresponds to the number in the kernel.\n",
    "To compute the output at the corresponding location (top left), we \"dot\" the\n",
    "pixel intensities in the square region with the kernel. That is, we perform\n",
    "the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(3 * 0 + 3 * 1 + 2 * 2) + (0 * 2 + 0 * 2 + 1 * 0) + (3 * 0 + 1 * 1 + 2 * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green grid (right) contains the *output* of this convolution layer.\n",
    "This output is also called an **output feature map**. The terms **feature**,\n",
    "and **activation** are interchangable. The output value on the top left\n",
    "of the green grid is consistent with the value we obtained by hand in Python.\n",
    "\n",
    "To compute the next activation value (say, one to the right of the previous output),\n",
    "we will shfit the superimposed kernel over by one pixel:\n",
    "\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/360/lec/w04/imgs/math_conv2.png\" width=\"300px\">\n",
    "\n",
    "The dark blue region is moved to the right by one pixel. We again dot\n",
    "the pixel intensities in this region with the kernel to get another 12, and continues to get 17, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 2\n",
    "\n",
    "Show how we get the value 19 in the output above (in cyan on the right of the figure in this section).\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note the shrinked output**: Here, we did not use **zero padding** (at the edges) so the output of this layers is shrinked by 1 on all sides. If the kernel size is $k=2m+1$, the output will be shrinked by $m$ on all sides so the width and height will be both reduced by $2m$.\n",
    "\n",
    "#### Convolutions with Multiple Input/Output Channels\n",
    "\n",
    "For a colour image, the kernel will be a **3-dimensional tensor**. This kernel will move through the input features just like before, and we \"dot\" the pixel intensities with the kernel at each region, exactly like before. This \"size of the 3rd (colour) dimension\" is called the **number of input channels** or **number of input feature maps**.\n",
    "\n",
    "We also want to detect multiple features, e.g., both horizontal edges and vertical edges. We would want to learn **many** convolutional filters on the same input. That is,\n",
    "we would want to make the same computation above using different kernels, like this:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/68/Conv_layer.png\" width=\"200px\">\n",
    "\n",
    "Each circle on the right of the image represents the output of a different kernel dotted\n",
    "with the highlighted region on the right. So, the output feature is also a 3-dimensional tensor.\n",
    "The size of the new dimension\n",
    "is called the **number of output channels** or **number of output feature maps**.\n",
    "In the picture above, there are 5 output channels.\n",
    "\n",
    "The `conv2D` layer expects as input a tensor in the format \"NCHW\", meaning that the dimensions of the tensor should follow the order:\n",
    "\n",
    "* batch size\n",
    "* channel\n",
    "* height\n",
    "* width\n",
    "\n",
    "Let us create a convolutional layer using `nn.Conv2d`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconv1 = nn.Conv2d(in_channels=3,  # number of input channels\n",
    "                 out_channels=7, # number of output channels\n",
    "                 kernel_size=5)  # size of the kernel, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emulate a batch of 32 colour images, each of size 128x128, like this:\n",
    "x = torch.randn(32, 3, 128, 128)\n",
    "y = myconv1(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tensor is also in the \"NCHW\" format. We still have 32 images, and 7 channels\n",
    "(consistent with `out_channels` of `conv`), and of size 124x124. If we added the appropriate\n",
    "padding to `conv`, namely `padding` = $m$ (the kernel_size: $2m+1$), then our output width and height should be consistent with the input width and height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myconv2 = nn.Conv2d(in_channels=3,\n",
    "                  out_channels=7,\n",
    "                  kernel_size=5,\n",
    "                  padding=2)\n",
    "\n",
    "x = torch.randn(32, 3, 128, 128)\n",
    "y = myconv2(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The parameters of `Conv2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conv_params = list(myconv2.parameters())\n",
    "print(\"len(conv_params):\", len(conv_params))\n",
    "print(\"Filters:\", conv_params[0].shape)  #7 filters, each of size 3 x 5 x 5\n",
    "print(\"Biases:\", conv_params[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers - Subsampling\n",
    "\n",
    "A pooling layer can be created like this: \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "y = myconv2(x)\n",
    "z = mypool(y)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the kernel size and the stride length will be equal so each pixel is pooled only once. \n",
    "The pooling layer has **no trainable parameters**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mypool.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 6, we did not define a class for our linear regression NN. Here we do so and define a CNN class consisting of several layers as defined below (from the official the Pytorch tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) #3: #input channels; 6: #output channels; 5: kernel size\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "myCNN = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__init__()` defines the layers.  `forward()` defines the *forward pass* that transform the input to the output. `backward()` is automatically defined using `autograd`. `ReLu()` is the [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), a popular **activation function** that performs a *nonlinear* transformation/mapping of an input variable (element-wise operation). `Conv2d()` defines a convolution layer, as shown below where blue maps indicate inputs, and cyan maps indicate outputs.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <td  style=\"text-align: left\"> Convolution with no padding, no strides.      <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "More convolution layers are illustrated nicely at [Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic). \n",
    "\n",
    "\n",
    "This network `CNN()` defined above has **two** convolutional layers: `conv1` and `conv2`.\n",
    "\n",
    "- The first convolutional layer `conv1` requires an input with 3 channels,\n",
    "  outputs **6 channels**, and has a kernel size of `5x5`. We are not adding any zero-padding.\n",
    "- The second convolutional layer `conv2` requires an input with **6 channels** (note this **MUST match the output channel number of the previous layer**),  outputs 16 channels, and has a kernel size of (again) `5x5`. We are not adding any zero-padding.\n",
    "\n",
    "In the `forward` function we see that the convolution operations are always \n",
    "followed by the usual ReLU activation function, and a pooling operation.\n",
    "The pooling operation used is max pooling, so each pooling operation\n",
    "**reduces the width and height of the neurons in the layer by half**.\n",
    "\n",
    "\n",
    "\n",
    "Because we are not adding any zero padding, we end up with `16 * 5 * 5` hidden units\n",
    "after the second convolutional layer (`16` matches the output channel number of `conv2`, `5 * 5` is based on the input dimension `32x32`, see below). These units are then passed to two fully-connected\n",
    "layers, with the usual ReLU activation in between.\n",
    "\n",
    "Notice that the number of channels **grew** in later convolutional layers! However,\n",
    "the number of hidden units in each layer is still reduced because of the convolution and pooling operation:\n",
    "\n",
    "* Initial Image Size: $3 \\times 32 \\times 32 $\n",
    "* After `conv1`: $6 \\times 28 \\times 28$ ($32 \\times 32$ is reduced by `2` on each side)\n",
    "* After Pooling: $6 \\times 14 \\times 14 $ (image size halved)\n",
    "* After `conv2`: $16 \\times 10 \\times 10$ ($14 \\times 14$ is reduced by `2` on each side)\n",
    "* After Pooling: $16 \\times 5 \\times 5 $ (halved)\n",
    "* After `fc1`: $120$\n",
    "* After `fc2`: $84$\n",
    "* After `fc3`: $10$ (**= number of classes**)\n",
    "\n",
    "This pattern of **doubling the number of channels with every pooling / strided convolution**\n",
    "is common in modern convolutional architectures. It is used to avoid loss of too much information within\n",
    "a single reduction in resolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 3\n",
    "\n",
    "If the input image size is $3 \\times 64 \\times 64 $, can we use the same CNN defined above? If yes, show the feature sizes after each operation as above. If no, how shall we modify the network architecture to process such $3 \\times 64 \\times 64 $ images?\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the NN architecture\n",
    "\n",
    "Now let's take a look at the CNN built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the (randomly initialised) parameters of this NN. Below, we check the first 2D convolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(myCNN.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # First Conv2d's .weight\n",
    "print(params[1].size())  # First Conv2d's .bias\n",
    "print(params[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4\n",
    "\n",
    "From the above, we can see the length of `params` is 10, i.e. there are 10 sets of parameters. Set 0 is for the weights of `conv1`. Set 1 is the bias of `conv1`. What are the remaining 8 sets for?\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about these functions, refer to the [`torch.nn` documentation](https://pytorch.org/docs/stable/nn.html) (search for the function, e.g., search for `torch.nn.ReLu` and you will find its documentation [here](https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimisation, Training and Testing\n",
    "\n",
    "### Choose a criterion (cross-entropy loss) and an optimizer (SGD with momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(myCNN.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network\n",
    "\n",
    "Next, we will feed data to this CNN to train it, i.e., learn its parameters so that the criterion above (cross-entropy loss) is minimised, using the SGD optimiser. The dataset is loaded in batches to train the model. One `epoch` means one cycle through the full training dataset.  The steps are \n",
    "* Define the optimisation criterion and optimisation method.\n",
    "* Iterate through the whole dataset in batches, for a number of `epochs` till a maximum specified or a convergence criteria (e.g., successive change of loss < 0.000001)\n",
    "* In each batch processing, we \n",
    "    * do a forward pass\n",
    "    * compute the loss\n",
    "    * backpropagate the loss via `autograd`\n",
    "    * update the parameters\n",
    "\n",
    "Now, we loop over our data iterator, and feed the inputs to the network and optimize. Here, I set `max_epochs` to 2 for quick testing. In practice, more epochs typically lead to better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs=2\n",
    "for epoch in range(max_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = myCNN(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at how `autograd` keeps track of the gradients for back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save our trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/cifar_net.pth'\n",
    "torch.save(myCNN.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://pytorch.org/docs/stable/notes/serialization.html)\n",
    "for more details on saving PyTorch models.\n",
    "\n",
    "### Test the network on the test data\n",
    "\n",
    "We will test the trained network by predicting the class label that the neural network outputs, and checking it against the ground-truth. Okay, first step. Let us display an image from the test set to get familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batchSize)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load back in our saved model (note: saving and re-loading wasn't necessary here, we only did it for illustration):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadCNN = CNN()\n",
    "loadCNN.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = loadCNN(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are energies for the 10 classes.\n",
    "The higher the energy for a class, the more the network\n",
    "thinks that the image is of the particular class.\n",
    "So, let's get the index of the highest energy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(batchSize)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate do you get? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 5\n",
    "\n",
    "Note that the results are *random* because the batches are randomly formed. How to fix the randomness so that you can get the same results if you run your code multiple times? \n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at how the network performs on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  #testing phase, no need to compute the gradients to save time\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = loadCNN(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something around 50%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 6\n",
    "\n",
    "What is the **chance level** accuracy (i.e. random classification)? How good is your CNN accuracy compared to the chance level?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your network works better than chance, it seems like to have learnt something.\n",
    "\n",
    "Hmmm, what are the classes that performed well, and the classes that did not perform well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = loadCNN(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(batchSize):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Additional ideas to explore\n",
    "\n",
    "* Change the [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) to different choices and compare the results.\n",
    "* Train the network for more passes (epochs) and/or other training settings (e.g., other patch sizes) to study performance variations.\n",
    "* Practice change the CNN architecture (pay attention to the size matching between different layers).\n",
    "* Try on GPU if you have one by following the end of the [the CIFAR10 Pytorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py).\n",
    "* Try out the AlexNet `torchvision.models.alexnet` and other architectures from `torchvision.models` for image classification\n",
    "* Try out other pytorch computer vision datasets `torchvision.datasets` for image classification. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
